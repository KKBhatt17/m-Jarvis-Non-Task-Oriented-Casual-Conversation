{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiYK0L7uEsl1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZFhNeMvtr31",
        "outputId": "17f60b61-a700-414c-e848-9f50660b4530"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSyyNpk-r79U",
        "outputId": "a906a982-235c-4581-a10b-08d4ada56791"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-5de3a29c-51ac-7016-3f52-d037bd817787)\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBQzHSyUsykF"
      },
      "source": [
        "Things to do - \n",
        "\n",
        "1. Setup an Embedding for the vocabulary.\n",
        "2. Setup the code for obtaining the positional encoding.\n",
        "3. Setup the code for a single encoder block.\n",
        "4. Setup the code for encoder segment of the seq2seq model.\n",
        "5. Setup the code for a single decoder block.\n",
        "6. Setup the code for the entire decoder segment.\n",
        "\n",
        "\n",
        "References - \n",
        "1. https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec\n",
        "2. https://www.tensorflow.org/text/tutorials/transformer#create_the_transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPM3IdV4u0Cj"
      },
      "source": [
        "But first, setting up a few conventions to ease the parallelization of code, while maintaining the constraint of passing forward context - \n",
        "\n",
        "1. Input to the Encoder would be of the form - \n",
        "    \n",
        "    A sentence, as an array of words (or their corresponding one-hot vectors).\n",
        "\n",
        "\n",
        "\n",
        "NOTE: In order to ignore Vanishing/Exploding gradients, I will be using a simple architecture that runs with a window of size `3` sentences, including the current sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_piT8zXTtn9j",
        "outputId": "19334e22-9e4d-4280-dcc5-d21c4a27d701"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conversational_AI.ipynb    dataset_no_ctxt_processed.pkl      vocab.json\n",
            "dataset_no_ctxt_lower.pkl  dataset_single_way_ctxt_lower.pkl  vocab_lower.json\n",
            "dataset_no_ctxt.pkl\t   dataset_single_way_ctxt.pkl\n"
          ]
        }
      ],
      "source": [
        "!cp -r drive/MyDrive/Datasets/Conv\\ AI\\ 2 datasets\n",
        "!ls datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZMfieLnF5td"
      },
      "outputs": [],
      "source": [
        "#Loading dataset\n",
        "dataset_path = 'datasets/dataset_no_ctxt_processed.pkl'\n",
        "\n",
        "dataset = None\n",
        "vocab_mapper = None\n",
        "with open(dataset_path, 'rb') as file:\n",
        "    dataset = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1BIEV4C22Yc"
      },
      "outputs": [],
      "source": [
        "with open('datasets/vocab.json', 'r') as file:\n",
        "    vocab_mapper = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3XPN9kHQKWU",
        "outputId": "839895bf-8329-4c0c-8ee5-dbf9f380034f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9641"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vocab_mapper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EF3QFg4cK5pq"
      },
      "outputs": [],
      "source": [
        "inverse_vocab_mapper = {idx: word for word, idx in vocab_mapper.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTQzaCk7KjaM",
        "outputId": "de01d95b-a99a-4729-bc8d-eeb5c4c27a90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[array([ 77, 120, 121, 122,  61], dtype=int32)],\n",
              " array([9639,  123,   12,  105,  106,   12], dtype=int32),\n",
              " array([ 123,   12,  105,  106,   12, 9640], dtype=int32)]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxNHcj_hsxex"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding:\n",
        "    def trig_encoding(self, pos, idx, val):\n",
        "        if idx%2 == 0:\n",
        "            # Sine encoding\n",
        "            return np.sin(pos/val)\n",
        "        else:\n",
        "            #Cosine encoding\n",
        "            return np.cos(pos/val)\n",
        "    def __call__(self, pos, d_model):\n",
        "        encodings_angle = np.array(list(map(lambda idx: np.power(10000, (2 * (idx//2))/d_model), range(d_model))))\n",
        "        encodings_trig = np.array(list(map(lambda idx_val: self.trig_encoding(pos, *idx_val), enumerate(encodings_angle))))\n",
        "\n",
        "        return encodings_trig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehY2Ic9XHENo"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.head_depth = d_model // num_heads\n",
        "\n",
        "        self.wq = keras.layers.Dense(units=d_model)\n",
        "        self.wk = keras.layers.Dense(units=d_model)\n",
        "        self.wv = keras.layers.Dense(units=d_model)\n",
        "\n",
        "        self.outputs = keras.layers.Dense(units=d_model)\n",
        "\n",
        "    def _create_mask(self, shape):\n",
        "        return 1 - np.tri(shape)\n",
        "\n",
        "    def _scaled_dot_product_attention(self, query, key, value, mask):\n",
        "        '''\n",
        "        Input shapes - \n",
        "        query: (num_heads, seq_len, head_depth)\n",
        "        key: (num_heads, seq_len, head_depth)\n",
        "        value: (num_heads, seq_len, head_depth)\n",
        "        '''\n",
        "\n",
        "        dot_prod_attn = tf.matmul(query, key, transpose_b=True)\n",
        "        dk = tf.cast(self.head_depth, tf.float32)\n",
        "        scaled_dot_prod_attn = dot_prod_attn / tf.math.sqrt(dk)\n",
        "        # Shape of scaled attn - (num_heads, seq_len_q, seq_len_k)\n",
        "        if mask is not None:\n",
        "            # Masking with a really small value, to get 0 prob\n",
        "            # as attention for these\n",
        "            miniscule_value_multiplier = -100000\n",
        "            scaled_dot_prod_attn += mask * miniscule_value_multiplier\n",
        "\n",
        "        # Softmax applied only on the last axis\n",
        "        # in order to maintain the fact that attn weights\n",
        "        # add up to 1 for a single query.\n",
        "        attn_weights = tf.nn.softmax(scaled_dot_prod_attn, axis=-1)\n",
        "\n",
        "        outputs = tf.matmul(attn_weights, value)\n",
        "        # Shape of outputs - (num_heads, seq_len, head_depth)\n",
        "        return outputs\n",
        "\n",
        "    def _split_head(self, data):\n",
        "        '''\n",
        "        Input shape - (seq_len, d_model)\n",
        "        Transformed shape - (num_heads, seq_len, head_depth)\n",
        "        '''\n",
        "        data = tf.reshape(data, (-1, self.num_heads, self.head_depth))\n",
        "        return tf.transpose(data, perm=[1, 0, 2])\n",
        "\n",
        "    def __call__(self, query, key, value, should_mask=False):\n",
        "        # Do the masking if needed.\n",
        "        # Get the query, key and value representations and split into heads\n",
        "        # Do the self attention\n",
        "        # Concat the results and pass them through the output layer\n",
        "\n",
        "        '''\n",
        "        Input shapes - \n",
        "        query: (seq_len, d_model)\n",
        "        key: (seq_len, d_model)\n",
        "        value: (seq_len, d_model)\n",
        "        '''\n",
        "        mask = None\n",
        "        if should_mask:\n",
        "            # Mask should be of the shape - (seq_len, seq_len)\n",
        "            mask = self._create_mask(query.shape[0])\n",
        "\n",
        "        query = self.wq(query)\n",
        "        key = self.wk(key)\n",
        "        value = self.wv(value)\n",
        "\n",
        "        # Splitting into multiple heads\n",
        "        query = self._split_head(query)\n",
        "        key = self._split_head(key)\n",
        "        value = self._split_head(value)\n",
        "\n",
        "        attn_output = self._scaled_dot_product_attention(query, key, value, mask) # (num_heads, seq_len, head_depth)\n",
        "        attn_output = tf.transpose(attn_output, perm=[1, 0, 2]) # (seq_len, num_heads, head_depth)\n",
        "        concatenated_attn = tf.reshape(attn_output, (-1, self.d_model)) # (seq_len, d_model)\n",
        "\n",
        "        outputs = self.outputs(concatenated_attn) # (seq_len, d_model)\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uxjojXwJ9ox"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNetwork(keras.layers.Layer):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "        self.model = keras.Sequential([\n",
        "            keras.layers.Dense(units=d_ff, activation='relu'),\n",
        "            keras.layers.Dense(units=d_model)\n",
        "        ])\n",
        "    \n",
        "    def __call__(self, inputs):\n",
        "        '''\n",
        "        input shape == (seq_len, d_model)\n",
        "        output shape == (seq_len, d_model)\n",
        "        '''\n",
        "        return self.model(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmtWTjk1J-kS"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, d_ff):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        epsilon = 1e-6\n",
        "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.layer_norm_1 = keras.layers.LayerNormalization(epsilon=epsilon)\n",
        "        self.ffn = FeedForwardNetwork(d_model, d_ff)\n",
        "        self.layer_norm_2 = keras.layers.LayerNormalization(epsilon=epsilon)\n",
        "\n",
        "    def __call__(self, x, prev_key=None, prev_value=None):\n",
        "        if prev_key == None:\n",
        "            attn_output = self.attn(x, x, x)\n",
        "        else:\n",
        "            # Where recurrence develops\n",
        "            attn_output = self.attn(x, prev_key, prev_value) # Based on the query, formed from previous encoding, and key and value from current input, attend to current input\n",
        "        \n",
        "        norm_output = self.layer_norm_1(x + attn_output)\n",
        "        ffn_output = self.ffn(norm_output)\n",
        "        norm_output_2 = self.layer_norm_2(ffn_output + norm_output)\n",
        "\n",
        "        return norm_output_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBiWP3tdqoC6"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, d_ff):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        epsilon=1e-6\n",
        "        self.masked_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.layer_norm_1 = keras.layers.LayerNormalization(epsilon=epsilon)\n",
        "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.layer_norm_2 = keras.layers.LayerNormalization(epsilon=epsilon)\n",
        "        self.ffn = FeedForwardNetwork(d_model, d_ff)\n",
        "        self.layer_norm_3 = keras.layers.LayerNormalization(epsilon=epsilon)\n",
        "\n",
        "    def __call__(self, x, enc_output):\n",
        "        masked_attn_op = self.masked_attn(x, x, x, should_mask=True) # --> Problem in masking\n",
        "        norm_output_1 = self.layer_norm_1(masked_attn_op + x)\n",
        "        enc_dec_attn = self.enc_dec_attn(norm_output_1, enc_output, enc_output) # query comes from the half target output, and key value come from enc_output\n",
        "        norm_output_2 = self.layer_norm_2(enc_dec_attn + norm_output_1)\n",
        "        ffn_output = self.ffn(norm_output_2)\n",
        "        norm_output_3 = self.layer_norm_3(ffn_output + norm_output_2)\n",
        "\n",
        "        return norm_output_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVckZlHiqsPB"
      },
      "outputs": [],
      "source": [
        "class Encoder(keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, d_ff, num_heads, input_vocab_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = keras.layers.Embedding(input_dim=input_vocab_size, output_dim=d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "\n",
        "        self.encoders = [EncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)]\n",
        "\n",
        "    def __call__(self, x, prev_hidden_encoding=None):\n",
        "        # Shape of x == (seq_len)\n",
        "        seq_len = len(x)\n",
        "        x = self.embedding(x) # --> (seq_len, d_model)\n",
        "        pos_enc = np.array([self.pos_encoding(i+1, self.d_model) for i in range(seq_len)]) # --> (seq_len, d_model)\n",
        "        x += pos_enc\n",
        "\n",
        "        for i, encoder in enumerate(self.encoders):\n",
        "            x = encoder(x)\n",
        "            # if i != self.num_layers - 1 or prev_hidden_encoding is None:\n",
        "            #     # Not the last encoder\n",
        "            #     x = encoder(x)\n",
        "            # else:\n",
        "            #     x = encoder(x, prev_key=prev_hidden_encoding, prev_value=prev_hidden_encoding)\n",
        "        return x # --> (seq_len, d_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_55UUlsPweqn"
      },
      "outputs": [],
      "source": [
        "class Decoder(keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, d_ff, num_heads, output_vocab_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = keras.layers.Embedding(input_dim=output_vocab_size, output_dim=d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "\n",
        "        self.decoders = [DecoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)]\n",
        "\n",
        "        self.final_layer = keras.layers.Dense(units=output_vocab_size, activation='softmax') # Final output -> probabilities, not logits\n",
        "\n",
        "\n",
        "    def predict_once(self, x, enc_output):\n",
        "        # x -> The output prepended and appended with [START] and [END] tokens\n",
        "        # shape of x --> (seq_len, )\n",
        "        # enc_output -> the output from the encoder\n",
        "        # shape of enc_output --> (seq_len, d_model)\n",
        "        seq_len = len(x)\n",
        "        x = self.embedding(x) # --> (seq_len, d_model)\n",
        "        pos_enc = np.array([self.pos_encoding(i+1, self.d_model) for i in range(seq_len)]) # --> (seq_len, d_model)\n",
        "        x += pos_enc\n",
        "\n",
        "        for decoder in self.decoders:\n",
        "            x = decoder(x, enc_output)\n",
        "\n",
        "        outputs = self.final_layer(x) # --> (seq_len, output_vocab_size) -> outputs prob for each place\n",
        "        return outputs\n",
        "\n",
        "        # last_token_probs = outputs[-1] # --> (output_vocab_size, )\n",
        "        # predicted_token_id = tf.argmax(last_token_probs, axis=-1) # Getting along the final axis \n",
        "\n",
        "        # predicted_token = untokenize(predicted_token_id) # Right now tokenizer is simple, a number -> word mapping\n",
        "        # return predicted_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckEanl9F0uUJ"
      },
      "outputs": [],
      "source": [
        "class Transformer(keras.Model):\n",
        "    def __init__(self, num_enc_layers, num_dec_layers, d_model, d_ff, num_heads, input_vocab_size, output_vocab_size):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(num_enc_layers, d_model, d_ff, num_heads, input_vocab_size)\n",
        "        self.decoder = Decoder(num_dec_layers, d_model, d_ff, num_heads, output_vocab_size)\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        # inp_sents are basically n prompts - n-1 prev and 1 current\n",
        "        # tar_half_sent is the sent that has been generated up until now\n",
        "        inp_sents, tar_half_sent = inputs\n",
        "\n",
        "        enc_output = self.encoder(inp_sents[0])\n",
        "\n",
        "        dec_output = self.decoder.predict_once(tar_half_sent, enc_output)\n",
        "        # returns probability for all the tokens in the window slid forward by 1 unit\n",
        "        # thus, the tar real has to be the real tokens in the window slid by 1 unit\n",
        "        # the probability on all the tokens in this window will be used for cross entropy computation\n",
        "        return dec_output "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcihFEt6n4-9"
      },
      "outputs": [],
      "source": [
        "loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "def loss_function(y_true, y_pred):\n",
        "    # y_true --> (seq_len, output_vocab_size) - one hot vectors\n",
        "    # y_pred --> (seq_len, output_vocab_size) - probabilities for each token in the sequence\n",
        "    return loss_object(y_true, y_pred)\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0l5ge1jcz1jG"
      },
      "outputs": [],
      "source": [
        "NUM_ENC_LAYERS = 2\n",
        "NUM_DEC_LAYERS = 2\n",
        "D_MODEL = 384\n",
        "D_FF = 512\n",
        "NUM_HEADS = 8\n",
        "INPUT_VOCAB_SIZE = len(vocab_mapper)\n",
        "OUTPUT_VOCAB_SIZE = len(vocab_mapper)\n",
        "NUM_EPOCHS = 10\n",
        "grad_apply_every = 128 # Gradient will be accumulated till 128 samples have been seen\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOXjaiUMzTy2"
      },
      "outputs": [],
      "source": [
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edITPCiFbG2C"
      },
      "outputs": [],
      "source": [
        "conversational_model = Transformer(num_enc_layers=NUM_ENC_LAYERS, num_dec_layers=NUM_DEC_LAYERS, \n",
        "                                   d_model=D_MODEL, d_ff=D_FF, num_heads=NUM_HEADS, \n",
        "                                   input_vocab_size=INPUT_VOCAB_SIZE, output_vocab_size=OUTPUT_VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1fW_VQ4dm6B"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"./drive/MyDrive/conv_ai_no_ctxt_model/checkpoints/train\"\n",
        "model_wt_path = \"./drive/MyDrive/conv_ai_no_ctxt_model/saved_model/saved_weights\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-6GGKESdftt"
      },
      "outputs": [],
      "source": [
        "# conversational_model.load_weights(model_wt_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hO2MBdas799a",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "ckpt = tf.train.Checkpoint(transformer=conversational_model,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=10)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print('Latest checkpoint restored!!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Qpr3PKGtn9s"
      },
      "outputs": [],
      "source": [
        "early_stopper = keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, verbose=1, restore_best_weights=True)\n",
        "flag_model_set = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uS7g-b-ftn9t",
        "outputId": "e01a8cde-783f-4929-9387-10491e526549"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on 20585 samples, validating on 1790 samples\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "validation_split=0.08\n",
        "\n",
        "data_size = len(dataset)\n",
        "validation_size = int(validation_split * data_size)\n",
        "    \n",
        "train_valid_set = dataset\n",
        "    \n",
        "train_dataset = train_valid_set[:-validation_size]\n",
        "validation_dataset = train_valid_set[-validation_size:]\n",
        "\n",
        "print('Training on {} samples, validating on {} samples\\n\\n'.format(len(train_dataset), len(validation_dataset)))\n",
        "\n",
        "def eval_model(model, valid_data):\n",
        "    num_instances = len(valid_data)\n",
        "    total_loss = 0\n",
        "\n",
        "    for data_inst in tqdm(valid_data, desc='Evaluating . . . '):\n",
        "        inp_sents, targ_half_sent, real_half_sent_output = data_inst\n",
        "        predictions = model((inp_sents, targ_half_sent))\n",
        "\n",
        "        loss = loss_function(real_half_sent_output, predictions)\n",
        "        total_loss += loss\n",
        "\n",
        "    return total_loss/num_instances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWXaqdtsddje",
        "outputId": "ab7e3817-fc4a-4eb9-fc42-225f65b75c4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on 20585 samples, validating on 1790 samples\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running through samples . . . : 100%|██████████| 161/161 [35:38<00:00, 13.28s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss after epoch 1 is : 3.097515106201172\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating . . . : 100%|██████████| 1790/1790 [02:11<00:00, 13.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loss on validation set after epoch 1 is : 3.429464817047119\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running through samples . . . : 100%|██████████| 161/161 [35:39<00:00, 13.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss after epoch 2 is : 2.978832721710205\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating . . . : 100%|██████████| 1790/1790 [02:12<00:00, 13.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loss on validation set after epoch 2 is : 3.3306047916412354\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running through samples . . . : 100%|██████████| 161/161 [35:31<00:00, 13.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss after epoch 3 is : 2.8827767372131348\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating . . . : 100%|██████████| 1790/1790 [02:10<00:00, 13.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loss on validation set after epoch 3 is : 3.3509202003479004\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running through samples . . . : 100%|██████████| 161/161 [35:34<00:00, 13.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss after epoch 4 is : 2.7967822551727295\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating . . . : 100%|██████████| 1790/1790 [02:11<00:00, 13.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loss on validation set after epoch 4 is : 3.235826015472412\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running through samples . . . : 100%|██████████| 161/161 [35:40<00:00, 13.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss after epoch 5 is : 2.722592353820801\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating . . . : 100%|██████████| 1790/1790 [02:11<00:00, 13.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loss on validation set after epoch 5 is : 3.2358322143554688\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running through samples . . . : 100%|██████████| 161/161 [35:36<00:00, 13.27s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss after epoch 6 is : 2.6513049602508545\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating . . . : 100%|██████████| 1790/1790 [02:11<00:00, 13.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loss on validation set after epoch 6 is : 3.2958812713623047\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running through samples . . . : 100%|██████████| 161/161 [35:45<00:00, 13.33s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss after epoch 7 is : 2.5846352577209473\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating . . . : 100%|██████████| 1790/1790 [02:12<00:00, 13.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loss on validation set after epoch 7 is : 3.304631233215332\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running through samples . . . : 100%|██████████| 161/161 [35:44<00:00, 13.32s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss after epoch 8 is : 2.5272650718688965\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating . . . : 100%|██████████| 1790/1790 [02:12<00:00, 13.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loss on validation set after epoch 8 is : 3.3137619495391846\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running through samples . . . : 100%|██████████| 161/161 [35:34<00:00, 13.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss after epoch 9 is : 2.478549003601074\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating . . . : 100%|██████████| 1790/1790 [02:13<00:00, 13.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loss on validation set after epoch 9 is : 3.3024024963378906\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running through samples . . . : 100%|██████████| 161/161 [35:33<00:00, 13.25s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss after epoch 10 is : 2.4342877864837646\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating . . . : 100%|██████████| 1790/1790 [02:08<00:00, 13.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loss on validation set after epoch 10 is : 3.272355318069458\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "logger_path = 'drive/MyDrive/logs/conv_ai_no_ctxt-train_logs.txt'\n",
        "LOGGER_TEMPLATE = 'Timestamp: {}, epochs done: {}, train loss: {}, val_loss: {}\\n'\n",
        "already_done = 10\n",
        "\n",
        "if not flag_model_set:\n",
        "    early_stopper.set_model(conversational_model)\n",
        "    early_stopper.best = conversational_model\n",
        "    flag_model_set = True\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    total_loss = 0\n",
        "    np.random.shuffle(dataset)\n",
        "    \n",
        "    if epoch == 0: print('Training on {} samples, validating on {} samples\\n\\n'.format(len(train_dataset), len(validation_dataset)))\n",
        "\n",
        "    for curr_inp_head in tqdm(range(0, len(train_dataset), grad_apply_every), desc='Running through samples . . . '):\n",
        "        batch_indices = list(range(curr_inp_head, curr_inp_head + grad_apply_every))\n",
        "        batch_indices = list(filter(lambda val: val < len(train_dataset), batch_indices))\n",
        "        avg_batch_loss = 0\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            for idx, batch_index in enumerate(batch_indices):\n",
        "                inp_sents, targ_half_sent, real_half_sent_output = train_dataset[batch_index]\n",
        "\n",
        "                predictions = conversational_model((inp_sents, targ_half_sent))\n",
        "                loss = loss_function(real_half_sent_output, predictions)\n",
        "                avg_batch_loss += loss/grad_apply_every\n",
        "                total_loss += loss\n",
        "\n",
        "        grads = tape.gradient(avg_batch_loss, conversational_model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, conversational_model.trainable_variables))\n",
        "\n",
        "    train_loss = total_loss/len(train_dataset)\n",
        "    print('Loss after epoch {} is : {}'.format(epoch + 1, train_loss))\n",
        "    val_loss = eval_model(conversational_model, validation_dataset)\n",
        "    print('\\nLoss on validation set after epoch {} is : {}\\n\\n'.format(epoch + 1, val_loss))\n",
        "    total_loss = 0\n",
        "        \n",
        "    ckpt_manager.save()\n",
        "\n",
        "    with open(logger_path, 'a') as file:\n",
        "        file.write(LOGGER_TEMPLATE.format(datetime.now(), epoch + 1 + already_done, train_loss, val_loss))\n",
        "    \n",
        "    try:\n",
        "        early_stopper.on_epoch_end(epoch, {'loss': train_loss, 'val_loss': val_loss})\n",
        "        if conversational_model.stop_training:\n",
        "            conversational_model.save_weights(model_wt_path)\n",
        "            ckpt_manager.save()\n",
        "            break\n",
        "    except:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHmxvbeuQaRU",
        "outputId": "08a75697-2a40-4fce-bfb6-b4dd66fa16da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=-9.0>"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.constant(-9, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vZs9659FC_b",
        "outputId": "8e97f28c-3db9-49e8-e661-c9d73c0358eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(13, 9640), dtype=float32, numpy=\n",
              "array([[1.60732074e-04, 9.92492278e-05, 7.16384384e-05, ...,\n",
              "        9.02669635e-05, 6.67647037e-05, 1.02540405e-04],\n",
              "       [1.57857285e-04, 1.00175806e-04, 7.25910431e-05, ...,\n",
              "        9.03166219e-05, 6.57833298e-05, 1.01242811e-04],\n",
              "       [1.55504691e-04, 1.01607919e-04, 7.22153127e-05, ...,\n",
              "        9.03519249e-05, 6.44348183e-05, 1.00991747e-04],\n",
              "       ...,\n",
              "       [1.56660011e-04, 1.03900406e-04, 6.97192736e-05, ...,\n",
              "        8.71837037e-05, 6.93367620e-05, 9.58213641e-05],\n",
              "       [1.53873378e-04, 1.04774677e-04, 6.99822485e-05, ...,\n",
              "        8.69799915e-05, 7.01670360e-05, 9.65382424e-05],\n",
              "       [1.48721228e-04, 1.05112449e-04, 7.08778171e-05, ...,\n",
              "        8.72736928e-05, 7.04818594e-05, 9.69216562e-05]], dtype=float32)>"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krh-zL1hOV5o",
        "outputId": "8fa658f8-6611-401d-9bac-8bea0c01646c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-9.466459"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.log(predictions[1][10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ve7ztSiMOj43",
        "outputId": "0ed9db2a-4fc8-4366-8466-11650d00c7d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-120.15122985839844"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJJSPsCHH9np",
        "outputId": "e47056a1-93d0-4757-c1db-8e8cb0e0fca1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([   9,   10,   11,   12,   13,    5,   14,   15,   16,   17,    8,\n",
              "         18, 9640], dtype=int32)"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "real_half_sent_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ty5wCnA1Nxub",
        "outputId": "21a74d5b-ba55-49f4-c04f-64b88fae8f98"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=nan>"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss_ = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "\n",
        "till=13\n",
        "\n",
        "loss_(real_half_sent_output[:till], predictions[:till])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqM_sxVbH1ou",
        "outputId": "1c4d0304-3042-4ad6-b643-3e424737715b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(6, 384), dtype=float32, numpy=\n",
              "array([[-0.04498158, -0.04677614, -0.00107815, ..., -0.04874101,\n",
              "        -0.01611261, -0.01227305],\n",
              "       [-0.00416957, -0.0283388 ,  0.03368807, ...,  0.03665916,\n",
              "        -0.01670926, -0.03746808],\n",
              "       [ 0.00083814, -0.01860614,  0.0231227 , ...,  0.02254893,\n",
              "         0.01429414,  0.00211374],\n",
              "       [ 0.00557966, -0.02820444, -0.04735374, ..., -0.02564529,\n",
              "         0.02055795, -0.04375667],\n",
              "       [ 0.00083814, -0.01860614,  0.0231227 , ...,  0.02254893,\n",
              "         0.01429414,  0.00211374],\n",
              "       [ 0.0296078 , -0.0244195 , -0.04876135, ..., -0.01753531,\n",
              "         0.03001041, -0.03542858]], dtype=float32)>"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emb(np.array([1, 165, 25, 484, 25, 205]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FuMbrdM0lFX"
      },
      "outputs": [],
      "source": [
        "with open(logger_path, 'a') as file:\n",
        "        file.write(LOGGER_TEMPLATE.format(datetime.now(), epoch + 1, train_loss, val_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "wLoyOAyS0mdp",
        "outputId": "fb3665ff-732f-41c5-b15a-f45a21909aa6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./drive/MyDrive/conv_ai_single_way_model_v2/checkpoints/train/ckpt-2'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ckpt_manager.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gc0xGrsdeFNz",
        "outputId": "e9812c97-96ce-4009-fe3b-dfefc103558c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[array([1, 2, 3, 4, 5, 6, 7, 8, 3, 4], dtype=int32),\n",
              "  array([ 1, 19, 20, 21, 22, 23,  5, 19, 20, 21, 22], dtype=int32)],\n",
              " array([9639], dtype=int32),\n",
              " array([1], dtype=int32)]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Evaluating the model\n",
        "dataset[12]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PssFI06RePfY"
      },
      "outputs": [],
      "source": [
        "inp_sents = dataset[12][0]\n",
        "targ_half_sent = dataset[12][1]\n",
        "preds = conversational_model((inp_sents, targ_half_sent))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8SzyLoFemRM",
        "outputId": "1bcb4143-3ee4-4deb-8b37-442580b4c3ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['i']"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(map(lambda idx: inverse_vocab_mapper[idx], tf.argmax(preds, axis=-1).numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNPDfRkhftkE",
        "outputId": "453aeb34-9097-42ef-fe92-8bdb97b53a51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['//START//']"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(map(lambda idx: inverse_vocab_mapper[idx], dataset[12][1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "matT06eHfPem",
        "outputId": "3b494760-085a-4615-8777-44a15328a81c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I']"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(map(lambda idx: inverse_vocab_mapper[idx], dataset[12][2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEongczxf8mO"
      },
      "outputs": [],
      "source": [
        "prompt = \"What do you do?\"\n",
        "targ_half_sent = '//START//'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgQyINo90ZOg",
        "outputId": "ebaa2046-3f00-4035-c967-f7d44a170652"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqYqy8l9gMYN"
      },
      "outputs": [],
      "source": [
        "prompt_data = []\n",
        "words_id = np.array(list(map(lambda word: vocab_mapper[word], word_tokenize(prompt))), dtype=np.int32)\n",
        "prompt_data.append([words_id])\n",
        "targ_half_sent = np.array([vocab_mapper[targ_half_sent]], dtype=np.int32)\n",
        "prompt_data.append(targ_half_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKiJgBi7gV_i"
      },
      "outputs": [],
      "source": [
        "preds = None\n",
        "preds = tf.argmax(conversational_model(prompt_data), axis=-1).numpy()\n",
        "MAX_LENGTH = 10\n",
        "\n",
        "len_ctr = 1\n",
        "while preds[-1] != vocab_mapper['//END//']:\n",
        "    prompt_data = []\n",
        "    prompt_data.append([words_id])\n",
        "    targ_half_sent = np.append(targ_half_sent, preds[-1]).astype(np.int32)\n",
        "    prompt_data.append(targ_half_sent)\n",
        "\n",
        "    preds = tf.argmax(conversational_model(prompt_data), axis=-1).numpy()\n",
        "\n",
        "    len_ctr += 1\n",
        "    if len_ctr > MAX_LENGTH: break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLOoxkvinejO",
        "outputId": "d1b02c97-ad20-4031-8535-9199276aa861"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I', 'am', 'a', 'student', '//END//']"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(map(lambda idx: inverse_vocab_mapper[idx], preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dgqsURPnebN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1CCQOFviD5J",
        "outputId": "0d4d1f71-c972-4ea6-e363-52f1aa76179b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hello']"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(map(lambda idx: inverse_vocab_mapper[idx], tf.argmax(preds, axis=-1).numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSFIT773lkuL",
        "outputId": "b9edd655-4db2-421f-a226-9bb6ced38d24"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([9639,   74], dtype=int32)"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.append(targ_half_sent, preds[-1]).astype(np.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjWETUJBmqvX",
        "outputId": "703d257e-9403-4002-d9eb-1c1b7d8de55a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9640"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_mapper['//END//']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Znf9abotn9t"
      },
      "source": [
        "```\n",
        "# Save the weights\n",
        "model.save_weights('./checkpoints/my_checkpoint')\n",
        "\n",
        "# Create a new model instance\n",
        "model = create_model()\n",
        "\n",
        "# Restore the weights\n",
        "model.load_weights('./checkpoints/my_checkpoint')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "vX_G__2q98KC",
        "outputId": "974f07bc-5077-4d02-8289-654a0946b4e0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./drive/MyDrive/conv_ai_model/checkpoints/train/ckpt-1'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ckpt_manager.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x47Uv2up-sh6"
      },
      "outputs": [],
      "source": [
        "file_path = 'tmp'\n",
        "conversational_model.save_weights(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AzhTQfmtn9u"
      },
      "outputs": [],
      "source": [
        "model = Transformer(num_enc_layers=NUM_ENC_LAYERS, num_dec_layers=NUM_DEC_LAYERS, \n",
        "                                   d_model=D_MODEL, d_ff=D_FF, num_heads=NUM_HEADS, \n",
        "                                   input_vocab_size=INPUT_VOCAB_SIZE, output_vocab_size=OUTPUT_VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVGXRk_Ztn9u"
      },
      "outputs": [],
      "source": [
        "model.load_weights('./checkpoints/saved_models/conv_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZzYzdTTtn9u"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}